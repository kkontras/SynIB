{
  "training_params": {
    "tdqm_disable": false,
    "wandb_disable": false,
    "batch_size": 4,
    "test_batch_size": 4
  },
  "early_stopping": {
    "end_of_epoch_check": true,
    "n_steps_stop": 50,
    "save_every_valstep": 10,
    "validate_every": 25
  },
  "optimizer": {
    "type": "Adaw",
    "learning_rate": 0.00001,
    "momentum": 0.9,
    "weight_decay": 0.00001,
    "beta1": 0.9,
    "beta2": 0.999
  },
  "dataset": {
      "data_split": {
        "fold": 0
      }
  },
  "model": {
    "model_class": "QwenVL_ScienceQA_Cached",
    "args": {
      "model_name": "Qwen/Qwen3-VL-2B-Instruct",
      "d_model": 512, "num_classes": 3, "fc_inner": 64, "dropout": 0.1,
      "cls_finetune": false,
      "cls_type": "linear",
      "clip_grad": true,
      "clip_value": 1.0,
      "bias_infusion": {
        "method": "false",
        "l": 0.1,
        "starting_epoch": 0,
        "ending_epoch": 1500,
        "use": true,
        "plot": false
      },
      "lora_config": {
      "use_lora": true,  "lora_r": 16, "lora_alpha": 32,  "lora_dropout": 0.05,  "lora_target_modules": ["q_proj", "v_proj"],  "lora_bias": "none", "lora_lr": 0.0002
      },
      "multi_loss": {
        "multi_supervised_w": {
             "combined": 0, "c": 0, "g": 0
          }
      }
    },
    "load_ongoing": false,
    "save_dir": "SynIBCache_LoRa_{}.pth.tar",
    "encoders": [
      {
        "model": "LinearHead_Qwen",
        "args": {
          "d_model": 2048,
          "num_classes": 3
        },
        "pretrainedEncoder": {
          "use": false,
          "dir": "Qwen3VL2B_LHead_fold{}.pth.tar"
        }
      }
    ]
  }
}